{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The ReCaS-Bari GPU Cluster","text":"<p>The GPU-cluster facility belongs to the ReCaS-Bari HPC cluster and provides better performance when GPU-based applications are executed. The cluster makes available 1755 cores, 13.7 TB RAM, 55 TB of disk space and 38 High performance GPU (18 Nvidia A100 and 20 Nvidia V100). Each node belonging to the cluster accesses  the whole ReCaS-Bari GPFS-based storage consisting of 3800 TB in single replica and 180TB in replica two. Node-storage bandwidth is 10 Gbps. Applications can be executed only via Docker container, which adds to the cluster easy configuration and customization, reliability, flexibility and security. Users can request the deployment of interactive remote IDE GPU-based services (Jupyter Notebook or RStudio) as well as the possibility to submit GPU-based workflow represented as Directed Acyclic Graphs (DAG). Whenever possible, services will be instantiated within a private network, in order to prevent external cyber attacks: in this case, users can use the ReCaS-Bari VPN to access the service. In order to exploit the GPU-cluster potentiality, users need to be registered to the service.</p>"},{"location":"guides/docker_and_dockerfile/","title":"Docker containers and Dockerfile","text":""},{"location":"guides/docker_and_dockerfile/#1-introduction","title":"1 Introduction","text":"<p>Docker is an open source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux. A Docker container packages an application with all of its dependencies into a standardized unit for software development.</p>"},{"location":"guides/docker_and_dockerfile/#2-building-custom-docker-containers-in-recas-bari","title":"2 Building custom Docker containers in ReCaS-Bari","text":"<p>The ReCaS Bari data center makes available a dedicated machine where to develop containers to be deployed on the HPC/GPU Cluster.</p> <p>This machine is accessible only to users with a ReCaS-Bari HPC/HTC account active. Users without such an account should register using this link (check the box \"Account for access to ReCas-Bari compute services (HTC/HPC)\").</p> <p>In order to access the machine, first access to the frontend using SSH in a command prompt/shell (Linux, Mac, and Windows \u226510) and insert your password when prompted.</p> <pre><code>ssh &lt;username&gt;@frontend.recas.ba.infn.it\n</code></pre> <p>After that, access the dedicated machine:</p> <pre><code>ssh &lt;username&gt;@tesla02.recas.ba.infn.it\n</code></pre> <p>Warning</p> <p>If you facing the \u201cToo Many Authentication Failures\u201d error, use the following command <code>ssh -o IdentitiesOnly=yes &lt;username&gt;@tesla02.recas.ba.infn.it</code></p>"},{"location":"guides/docker_and_dockerfile/#3-docker-most-important-commands","title":"3 Docker most important commands","text":"<p>In this document, the most important aspects of Docker will be covered.</p>"},{"location":"guides/docker_and_dockerfile/#31-docker-pull","title":"3.1 Docker pull","text":"<p>The <code>pull</code> command fetches an image from the Docker registry (a place where the docker images are stored and can be downloaded/pulled) and saves it to a host for using it.</p> <p><pre><code>docker pull ubuntu:20.04\n</code></pre> Output: <pre><code>20.04: Pulling from library/ubuntu\n345e3491a907: Pull complete\n57671312ef6f: Pull complete\n5e9250ddb7d0: Pull complete\nDigest: sha256:cf31af331f38d1d7158470e095b132acd126a7180a54f263d386da88eb681d93\nStatus: Downloaded newer image for ubuntu:20.04\ndocker.io/library/ubuntu:20.04\n</code></pre></p> <p>You can use the <code>docker image ls</code> command to list all images on your system.</p> <p><pre><code>docker image ls\n</code></pre> Output: <pre><code>REPOSITORY   TAG    IMAGE ID      CREATED         SIZE\nubuntu      20.04   7e0aa2d69a15   3 weeks ago   72.7MB\n</code></pre></p>"},{"location":"guides/docker_and_dockerfile/#32-docker-run","title":"3.2 Docker run","text":"<p>Now that the <code>ubuntu:20.04</code> Docker image is on your host, you are able to run it using the command:</p> <pre><code>docker run ubuntu:20.04\n</code></pre> <p>As you can see, this command does nothing. When you execute <code>docker run</code>, the Docker client finds the image (ubuntu:20.04 in this case), loads up the container and then runs a command in that container. When we ran <code>docker run ubuntu:20.04</code>, we didn't provide a command, so the container booted up, ran an empty command and then exited.</p> <p><pre><code>docker run ubuntu:20.04 echo \"My first command in a container\"\n</code></pre> Output: <pre><code>My first command in a container\n</code></pre></p> <p>As you can see, an output has been printed.</p> <p>If you want to execute multiple commands inside the container, it is possible open a terminal inside it.</p> <pre><code>docker run -it ubuntu:20.04 /bin/bash\nls /\nbin  boot  dev  etc  home  lib  lib32  lib64  libx32  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\nuptime\n15:37:57 up  7:20,  0 users,  load average: 0.00, 0.03, 0.05\nexit\n</code></pre>"},{"location":"guides/docker_and_dockerfile/#33-docker-ps","title":"3.3 Docker ps","text":"<p>The <code>docker ps</code> command shows you all containers that are currently running.</p> <pre><code>docker ps\nCONTAINER ID   IMAGE    COMMAND   CREATED   STATUS  PORTS   NAMES\n</code></pre> <p>Since no containers are running, we see a blank line. Let's try a more useful variant:</p> <pre><code>docker ps -a\nCONTAINER ID   IMAGE        COMMAND                 CREATED         STATUS                      PORTS   NAMES\nafcac6f85b26   ubuntu:20.04   \"bash\"                3 minutes ago   Exited (130) 4 seconds ago          blissful\\_chaum\nee288eeb2a67   ubuntu:20.04   \"echo 'My first comm\u2026\"   4 minutes ago   Exited (0) 4 minutes ago             flamboyant\\_spence\nfac38035f7bc   ubuntu:20.04   \"/bin/bash\"           6 minutes ago   Exited (0) 6 minutes ago            peaceful\\_kepler\n</code></pre> <p>The <code>-a</code> flag is used to show all the containers (while the default shows just the ones currently running), so what we see above is a list of all containers that we ran (and not deleted) up to now. Please notice that the STATUS column shows that these containers exited a few minutes ago.</p>"},{"location":"guides/docker_and_dockerfile/#34-docker-container-rm","title":"3.4 Docker container rm","text":"<p>To remove non-running containers, use this command:</p> <pre><code>docker container rm afcac6f85b26 ee288eeb2a67 fac38035f7bc\nafcac6f85b26\nee288eeb2a67\nfac38035f7bc\n</code></pre> <p>You can check using <code>docker ps -a</code> that the selected container is not on the host anymore</p> <pre><code>docker ps -a\nCONTAINER ID   IMAGE    COMMAND   CREATED   STATUS  PORTS   NAMES\n</code></pre> <p>If you want remove a container immediately after having executed some commands inside it you can use the <code>--rm</code> flag:</p> <pre><code>docker run -it --rm ubuntu:20.04 /bin/bash\nls /\nbin  boot  dev  etc  home  lib  lib32  lib64  libx32  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\nexit\n\ndocker ps -a\nCONTAINER ID   IMAGE    COMMAND   CREATED   STATUS  PORTS   NAMES\n</code></pre> <p>Periodical clean up of docker images</p> <p>The machine <code>tesla02.recas.ba.infn.it</code> is used ONLY to test container to be deployed on the HPC/GPU Cluster. Periodically all saved container images will be removed. Be aware of it.</p>"},{"location":"guides/docker_and_dockerfile/#35-docker-exec","title":"3.5 Docker exec","text":"<p>Docker gives the possibility to force a container in the background even if no command is passed.</p> <p>For example:</p> <pre><code>docker run -d -it ubuntu:20.04\n048f685e181b2064538d482fa48fc553aa6821b5f51d254e79aca8a5b6069834\n</code></pre> <p>The <code>-d</code> flag tells Docker to DETACH the container and run it in background and to print its ID. Since the container is still running, <code>docker ps</code> prints its information</p> <pre><code>docker ps\nCONTAINER ID   IMAGE        COMMAND     CREATED         STATUS      PORTS   NAMES\n048f685e181b   ubuntu:20.04   \"/bin/bash\"   4 seconds ago   Up 3 seconds            laughing_proskuriakova\n</code></pre> <p>Using <code>docker exec</code> is possible to enter inside the container with a shell:</p> <pre><code>docker exec -it 048f685e181b /bin/bash\nhostname\n048f685e181b\n</code></pre>"},{"location":"guides/docker_and_dockerfile/#36-docker-port-mapping","title":"3.6 Docker port mapping","text":"<p>If inside the container there is a service exposing a specific port, you need to map that port (inside the container) to a port on the host where the container is run. In order to do that, use the <code>-p</code> flag in <code>docker run</code>:</p> <pre><code>docker run -p 11111:12345 python:3 python3 -m http.server 12345\n</code></pre> <p>The above command executes a http server using the container image <code>python:3</code>. The server is linked to the port 12345 inside the container. Using -p 11111:12345, you are mapping the port 11111 on the host to the port 12345 of the container. So, in order to access the served web page you need to connect to <code>tesla02.recas.ba.infn.it:11111</code> using your browser.</p>"},{"location":"guides/docker_and_dockerfile/#4-recas-docker-registry","title":"4 ReCaS Docker registry","text":"<p>ReCaS provides a dedicated Docker registry for containers to be used on the GPU cluster.</p> <p>On the <code>tesla02</code> machine, login to the docker registry using the following command:</p> <pre><code>docker login registry-clustergpu.recas.ba.infn.it\nUsername: &lt;type your ReCaS-Bari HPC/HTC account username&gt;\nPassword: &lt;type your ReCaS-Bari HPC/HTC account password&gt;\nLogin Succeeded\n</code></pre> <p>The followings commands allow to store an official Docker image from Docker Hub (the official and public Docker registry) to the local ReCaS Docker registry:</p> <pre><code>docker pull ubuntu\ndocker image tag ubuntu registry-clustergpu.recas.ba.infn.it/&lt;your username&gt;/myubuntu:0.1\ndocker push registry-clustergpu.recas.ba.infn.it/&lt;your username&gt;/myubuntu:0.1\ndocker pull registry-clustergpu.recas.ba.infn.it/&lt;your username&gt;/myubuntu:0.1\n</code></pre> <p>Storage quota</p> <p>Each user has a storage quota on the ReCaS-Bari container registry. There is a limit of number of different container images you can save.</p> <p>Tip</p> <p>Using the same name (such as <code>myubuntu:0.1</code>) the pushing docker image will overwrite the existing one.</p>"},{"location":"guides/docker_and_dockerfile/#5-dockerfile","title":"5 Dockerfile","text":"<p>Using the commands <code>docker run -d</code> and <code>docker exec</code> is possible to execute a container in detached mode and execute commands inside it, e.g. to create files or install packages. Docker also gives the possibility to save this customized container. Unfortunately this kind of containers can not be deployed in the ReCaS GPU cluster. Dockerfile is the solution for this issue.</p> <p>A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image. It's a simple way to automate the image creation process. An advantage is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This makes Dockerfiles easier to write.</p> <p>Let\u2019s start with an example. The goal is to customize the Docker image <code>python:3.9.9-slim</code> in order to make it run with your ReCaS account user, needed in order to access your personal directory on ReCaS GPFS, and your desired python module.</p> <p>In order to do so, access  the <code>tesla02.recas.ba.infn.it</code> machine, create a dedicated directory:</p> <pre><code>mkdir /home/&lt;username&gt;/my_python3.9\ncd /home/&lt;username&gt;/my_python3.9\n</code></pre> <p>Use your favorite editor and create a file named <code>requirements</code> and copy this content inside.</p> <pre><code>pandas\nnumpy\ntables\nmatplotlib\n</code></pre> <p>In this file you should put a list of python modules you want to install in the container.</p> <p>Now, create another file named <code>Dockerfile</code> and copy the following content inside it.</p> <pre><code>FROM python:3.9.9-slim\n# User section (Mandatory)\nENV USERNAME=&lt;your username&gt;\nENV USERID=&lt;your user-id&gt;\nENV GROUPID=&lt;your group-id&gt;\n\nRUN groupadd -g $GROUPID $USERNAME &amp;&amp; adduser --disabled-password --gecos '' --uid $USERID --gid $GROUPID $USERNAME\n\nUSER $USERNAME\n# python modules installation\nADD requirements /home/$USERNAME/requirements\nRUN python3 -m pip install -r /home/$USERNAME/requirements\n</code></pre> <p>Let\u2019s explain the Dockerfile.</p>"},{"location":"guides/docker_and_dockerfile/#51-from-statement","title":"5.1 FROM Statement","text":"<p><code>FROM python:3.9.9-slim</code></p> <p>The <code>FROM</code> statement defines the Docker image to be used as a starting point. Every Dockerfile starts with the <code>FROM</code> statement.</p>"},{"location":"guides/docker_and_dockerfile/#52-user-creation-section","title":"5.2 User creation section","text":"<pre><code># User section (Mandatory)\nENV USERNAME=&lt;your username&gt;\nENV USERID=&lt;your user-id&gt;\nENV GROUPID=&lt;your group-id&gt;\nRUN groupadd -g $GROUPID $USERNAME &amp;&amp; adduser --disabled-password --gecos '' --uid $USERID --gid $GROUPID $USERNAME\n</code></pre> <p>The above section is MANDATORY and creates your user inside the container in order to access your data on the ReCaS-Bari GPFS file system. (/lustre/home and /lustrehome directories).</p> <p>If you don\u2019t know your userid and/or groupid, execute the following command on <code>tesla02.recas.ba.infn.it</code>:</p> <p><code>id &lt;username&gt;</code></p> <p>And get the numbers, not names/words.</p>"},{"location":"guides/docker_and_dockerfile/#53-user-statement","title":"5.3 USER Statement","text":"<p><code>USER $USERNAME</code></p> <p>The <code>USER</code> statement sets the username for any commands that follow it in the Dockerfile.</p>"},{"location":"guides/docker_and_dockerfile/#54-python-module-installation","title":"5.4 Python module installation","text":"<pre><code>ADD requirements /home/$USERNAME/requirements\nRUN python3 -m pip install -r /home/$USERNAME/requirements\n</code></pre> <p>In above lines, the <code>requirements</code> file is copied in the container using the <code>ADD</code> statement and the python modules listed inside it are installed.</p>"},{"location":"guides/docker_and_dockerfile/#55-docker-container-building-and-pushing","title":"5.5 Docker container building and pushing","text":"<p>Now all files have been written and you are ready to build your customized tensorflow docker image using the following command</p> <p><code>docker build -t registry-clustergpu.recas.ba.infn.it/&lt;username&gt;/mypython3.9:0.1 .</code></p> <p>Finally, upload the image to the docker registry:</p> <p><code>docker push registry-clustergpu.recas.ba.infn.it/&lt;username&gt;/mypython3.9:0.1</code></p>"},{"location":"guides/docker_and_dockerfile/#6-use-case-use-a-custom-docker-image-with-chronos","title":"6 Use case: use a custom docker image with Chronos","text":"<p>Prefer to this guide for more details on Chronos.</p> <p>Create a JSON file in your HOME directory in the ReCaS-Bari Storage (e.g. <code>/lustrehome/&lt;username&gt;/my-python3.9-job.json</code>) containing:</p> <pre><code>{\n  \"name\": \"&lt;username&gt;-my-python3.9-job\",\n  \"command\": \"python3 /lustrehome/&lt;username&gt;/my_script.py &gt; /lustrehome/&lt;username&gt;/my_script.output\",\n  \"shell\": true,\n  \"retries\": 4,\n  \"description\": \"\",\n  \"cpus\": 1,\n  \"disk\": 0,\n  \"mem\": 1024,\n  \"gpus\": 0,\n  \"environmentVariables\": [],\n  \"arguments\": [],\n  \"runAsUser\": \"&lt;username&gt;\",\n  \"owner\": \"&lt;username&gt;\",\n  \"ownerName\": \"&lt;username&gt;\",\n  \"container\": {\n    \"type\": \"mesos\",\n    \"image\": \"registry-clustergpu.recas.ba.infn.it/&lt;username&gt;/mypython3.9:0.1\",\n    \"volumes\": [{\"containerPath\": \"/lustrehome/&lt;username&gt;\", \"hostPath\": \"/lustrehome/&lt;username&gt;\", \"mode\": \"RW\"}]\n  },\n  \"schedule\": \"R1//P1Y\"\n}\n</code></pre> <p>Create a python script in your HOME directory in the ReCaS-Bari Storage (e.g. <code>/lustrehome/&lt;username&gt;/my_script.py</code>) containing: </p> <pre><code>import pandas as pd\ndata = {\n    'apples': [3, 2, 0, 1], \n    'oranges': [0, 3, 7, 2]\n}\ndf = pd.DataFrame(data)\nprint(df.shape)\n</code></pre> <p>Finally, you can submit the job using the folling command:</p> <pre><code>bash submit_chronos /lustrehome/&lt;username&gt;/my-python3.9-job.json\n</code></pre> <p>Wait few seconds and the file <code>/lustrehome/&lt;username&gt;/my_script.output</code> should appear in your directory. Watch it using:</p> <p><code>cat /lustrehome/&lt;username&gt;/my_script.output</code></p>"},{"location":"guides/jupyter-lab/","title":"Jupyter Lab Guide","text":"<p>Updated on 19 May 2025</p>"},{"location":"guides/jupyter-lab/#0-user-support","title":"0 User Support","text":"<p>If you need support for your application, please use this link to create ag ticket titled \u201cReCaS HPC/GPU: JupyterLab support\u201d and describe your issue.</p> <p>Please provide all the necessary information to help us resolve your issue more easily, such as the notebook absolute path, screenshots, and so on.</p> <p>It is STRONGLY advised to subscribe to the recas-hpu-gpu mailing list. Create a ticket with the title \u201cReCaS HPC/GPU: subscribe to the mailing list\u201d.</p>"},{"location":"guides/jupyter-lab/#1-introduction","title":"1 Introduction","text":"<p>JupyterLab is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modelling, data visualization, machine learning, and much more.</p> <p>The ReCaS JupyterLab can access your files and directories stored in the ReCaS-Bari storage (based on the GPFS distributed file system) and use high-performance GPUs to speed up the execution of your application.</p>"},{"location":"guides/jupyter-lab/#2-jupyterlab-launcher","title":"2 JupyterLab Launcher","text":"<p>This is the latest launcher version, implemented in the ReCaS-Bari JupyterHub.</p> <p></p>"},{"location":"guides/jupyter-lab/#21-jupyter-kernels","title":"2.1 Jupyter Kernels","text":"<p>The first two rows show all the available kernels in your JupyterLab instance.</p> <p>Initially, you will find only the Python 3 kernel.</p>"},{"location":"guides/jupyter-lab/#r-kernel","title":"R kernel","text":"<p>To install the R kernel, please execute the following command:</p> <pre><code>conda create --name r_kernel -c r r-irkernel r-essentials r-recommended -y \nconda activate r_kernel`\n</code></pre>"},{"location":"guides/jupyter-lab/#root-kernel","title":"Root kernel","text":"<p>To install the ROOT kernel, please execute the following command:</p> <pre><code>conda create -c conda-forge --name root_kernel root ipykernel -y\n</code></pre> <p>Some issues related to the Python bindings are known.</p> <p>You can use ROOT in interactive mode using the Terminal, after activating the conda environment with the following command:</p> <p><code>conda activate root_kernel</code></p>"},{"location":"guides/jupyter-lab/#22-utilities","title":"2.2 Utilities","text":"<p>The third row contains several utilities, such as Terminal (you can use it to execute bash commands in the Bash shell), and text, Markdown, Python, and R file editors.</p>"},{"location":"guides/jupyter-lab/#23-recas-bari-services-guides","title":"2.3 ReCaS-Bari Services Guides","text":"<p>In the bottom rows, you will find links to guides for all ReCaS-Bari GPU cluster services.</p>"},{"location":"guides/jupyter-lab/#3-installing-new-python-modules","title":"3 Installing new Python modules","text":""},{"location":"guides/jupyter-lab/#31-pip","title":"3.1 pip","text":"<p><code>pip</code> can be used to install new Python packages. </p> <p>Specific lines can be put inside the code, like followings:</p> <pre><code># Installation\n%pip install graphviz\n\n# Import \nimport graphviz\n</code></pre> <p>Warning</p> <p>Remember to put a <code>%</code> before the <code>pip install</code> command: in this way the python module is directly available in the kernel. Otherwise restart the kernel to use the installed packages.</p> <p>There is a drawback to use <code>pip install</code>: when your JupyterLab instance is restarted, you will lose all installed Python packages.</p>"},{"location":"guides/jupyter-lab/#32-conda","title":"3.2 conda","text":"<p>Alternately you can use also <code>conda</code> and install packages using the Terminal. To use the terminal in JupyterLab, open a new tab and select <code>Terminal</code> from the launcher.</p> <p>The following lines show how to install <code>pandas</code> package in a new conda environment :</p> <pre><code>conda create --name my_kernel pandas=2 -y\nconda activate my_kernel\n</code></pre> <p>After the second command, you will notice appearing the <code>(my_kernel)</code> in the shell.</p> <p>To test the installation, execute the following command</p> <pre><code>python -c 'import pandas; print(pandas.__version__)'\n</code></pre> <p>Conda will install the Python modules you need for your code/project in your Home directory in <code>/lustrehome</code>, enabling multiple advantages:</p> <ul> <li> <p>Your conda environment is always available even if your JupyterLab instance (also known as a container) is stopped or restarted.</p> </li> <li> <p>You can create a new Jupyter kernel, allowing a notebook to be executed within a specific conda environment.</p> </li> <li> <p>It is the first step to submit a notebook to the GPU cluster for batch execution.</p> </li> </ul> <p>Note</p> <p>Consider <code>mamba</code> instead of conda.</p>"},{"location":"guides/jupyter-lab/#321-troubleshooting","title":"3.2.1 Troubleshooting","text":""},{"location":"guides/jupyter-lab/#conda-init","title":"conda init","text":"<p>If conda shows you an \"conda init\" error, execute the command <code>conda init</code> and then execute the command:</p> <pre><code>source /lustrehome/&lt;username&gt;/.bashrc\n</code></pre>"},{"location":"guides/jupyter-lab/#base-conda-environment","title":"base conda environment","text":"<p>You cannot install python packages in the <code>base</code> conda environment, you have to create a new one as described above</p>"},{"location":"guides/jupyter-lab/#322-create-new-jupyter-kernel","title":"3.2.2 Create new Jupyter kernel","text":"<p>Starting from a conda environment it is possible to create a Jupyter kernel: this will make easier to execute a given notebook using a particular conda environment.</p> <p>Following some commands to create a conda environment and how to create a jupyter kernel linked to it:</p> <pre><code>conda create --name my_kernel -c conda-forge pandas=2 ipykernel -y\nconda activate my_kernel\npython -m ipykernel install --name my_kernel --user --display-name my_kernel\n</code></pre> <p>Remind to reload the browser in order to see your new kernel in the Jupyter launcher.</p> <p></p> <p>Once the kernel appears in the launcher, it is possible to select it inside the notebook, as shown in the following image.</p> <p></p>"},{"location":"guides/jupyter-lab/#4-upload-file-from-local-file-system","title":"4 Upload file from local file system","text":"<p>To upload files from your local file system to the JupyterLab workspace, press the button shown in the following figure and select the files to would like to upload.</p> <p></p>"},{"location":"guides/jupyter-lab/#5-enable-resource-usage-dashboards","title":"5 Enable Resource Usage Dashboards","text":"<p>The ReCaS JupyterLab allows you to monitor the resource usage of your application in real-time.</p> <p>A few simple steps are needed to create your personal dashboard.</p> <p>First, click the third tab on the left, named \"System Dashboards\", as shown in the figure below.</p> <p></p> <p>Second, double-click the desired blue button, and it will be placed as a tab near the notebook tab. Drag and move it to select your preferred position.</p> <p>The following figure shows the \"Machine Resources\" plots.</p> <p></p> <p>Additional plots can be displayed on the screen.</p> <p>The following figure also shows the \"GPU Resources\" plots.</p> <p></p>"},{"location":"guides/jupyter-lab/#6-dask","title":"6 Dask","text":"<p>Dask is a flexible open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: <code>Pandas</code>, <code>Scikit-learn</code> and <code>NumPy</code>. It also exposes low-level APIs that help programmers run custom algorithms in parallel. (Wikipedia)</p> <p>Dask allows you to split the workload among multiple workers. So, the first step involves creating a cluster. Workers can be run on different machines or on the same machine.</p>"},{"location":"guides/jupyter-lab/#61-start-a-new-dask-cluster","title":"6.1 Start a new Dask cluster","text":"<p>The easiest way to create a Dask cluster is through the following lines:</p> <pre><code>from dask.distributed import Client, LocalCluster\ncluster = LocalCluster()  # Launches a scheduler and workers locally\nclient = Client(cluster)  # Connect to distributed cluster and override default\n</code></pre> <p>After that, computation on Dask DataFrames can be executed. </p> <p>In the following line is shown the evaluation of the sum of the column <code>x</code> of the dataframe <code>df</code>.</p> <pre><code>df.x.sum().compute()  # This now runs on the distributed system\n</code></pre> <p>It is recommended to read the official Dask guide to learn all provided capabilities.</p>"},{"location":"guides/jupyter-lab/#62-enable-dask-resource-usage-dashboards","title":"6.2 Enable Dask Resource Usage Dashboards","text":"<p>Dash provides additional graphical objects to monitor in real-time application information like resource usage and application progress.</p> <p>Firstly, the creation of a Dask cluster through the Jupyter Lab interface. </p> <p>This is done by selecting the forth tabs on the left, the <code>Dask</code> Tab, and create a new cluster by clicking on the <code>NEW</code> button, as shown in the following figure.</p> <p></p> <p>Secondly, the importing of the created cluster in the code.</p> <p>This is easily done by clicking the <code>&lt; &gt;</code> button on the bottom left after having selected the cell inside the notebook, as shown in the following figure.</p> <p></p> <p>Following it is copied the generated code for convenience.</p> <pre><code>from dask.distributed import Client\n\nclient = Client(\"tcp://127.0.0.1:44359\")\nclient\n</code></pre> <p>Multiple Dask graphical objects can be enabled and moved inside the window.</p> <p>If the code exploit Dask objects, during its execution the information about the application (resources usage, application progress, ...) are shown in real-time, as shown in the following figure.</p> <p></p>"},{"location":"guides/jupyter-lab/#7-bash-process-management","title":"7 Bash process management","text":"<p>Your JupyterLab instace is started on a Linux Operative System. </p> <p>Learn how to interact with its Process management it is important to manage aspects of your application executions. </p> <p>Please, take a look of the most important commands at this link</p>"},{"location":"guides/jupyter-lab/#8-execute-a-jupyter-notebook-in-background-with-papermill","title":"8 Execute a Jupyter Notebook in background with papermill","text":"<p>Papermill is a tool for parameterizing and executing Jupyter Notebooks.</p> <p>Papermill lets you:</p> <ul> <li> <p>parameterize notebooks</p> </li> <li> <p>execute notebooks</p> </li> </ul> <p>Papermill is already installed on your JupyterLab instance.</p> <p>If you would like to use <code>papermill</code> also in your conda environment, please install it using the following command</p> <pre><code>conda install -c conda-forge papermill -y\n</code></pre> <p>Please task a look of the official documentation.</p>"},{"location":"guides/jupyter-lab/#9-rapids","title":"9 RAPIDS","text":"<p>The RAPIDS suite of open source software libraries gives you the freedom to execute end-to-end data science and analytics pipelines entirely on GPUs. </p> <p>Seamlessly scale from GPU workstations to multi-GPU servers and multi-node clusters with Dask.</p> <p>Accelerate your Python data science toolchain with minimal code changes and no new tools to learn.</p> <p>RAPIDS is an open source project. Supported by NVIDIA, it also relies on Numba, Apache Arrow, and many more open source projects. </p> <p>Reference</p>"},{"location":"guides/jupyter-lab/#10-useful-links","title":"10 Useful links","text":"<p>Dask Webpage</p> <p>Dask GitHub page</p> <p>RAPIDS Webpage</p> <p>RAPIDS Docker container Webpage</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users-k8s/","title":"JupyterHub for ReCaS users","text":"<p>Updated on 19 May 2025</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users-k8s/#important-user-support","title":"IMPORTANT: User Support","text":"<p>If you need support for your application, please use this link to create a ticket with title \u201cReCaS HPC/GPU: JupyterHub support\u201d and then describe your issue.</p> <p>It is STRONGLY advised to subscribe to the recas-hpu-gpu mailing list. Create a ticket with the title \u201cReCaS HPC/GPU: subscribe to the mailing list\u201d.</p> <p>Important messages will be sent ONLY using the mailing list.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users-k8s/#1-introduction","title":"1) Introduction","text":"<p>The ReCaS JupyterHub provides JupyterLab instances for ReCaS users. JupyterLabs are open-source web applications that allow you to create and share documents that contain live code, equations, visualisations, and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modelling, data visualisation, machine learning, and much more.</p> <p>JupyterLab instances served through this service can access your files and directories stored in the ReCaS-Bari storage (based on the GPFS distributed file system) and use high-performance GPUs to speed up the execution of your application.</p> <p>Only registered users can access this service. In the request section, you can find information on how to request the service.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users-k8s/#2-access-to-the-service","title":"2) Access to the service","text":"<p>The ReCaS JupyterHub service provides JupyterLab instances with or without GPU Hardware Accelerators, depending on whether you are authorized to use them.</p> <p>To access the ReCaS JupyterHub service, use the link https://hub.recas.ba.infn.it</p> <p>Note</p> <p>You may need to \"Clear cookies and site data\".</p> <p>Users authorized to use a JupyterLab instance with a GPU will only be provided with a Virtual GPU.</p> <p>Those interested in using a whole GPU must provide a STRONG motivation.</p> <p>Once you have clicked on the link, the authentication page will be shown:</p> <p></p> <p>Here you should enter your personal username and password, created during the registration phase.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users-k8s/#3-configuration-section","title":"3) Configuration section","text":"<p>After authenticated, you will have the possibility to configure your jupyterLab instance. The following image shows the configuration page.</p> <p></p> <p>As you can see, you can configure the following fields.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users-k8s/#31-cpus","title":"3.1) CPUs","text":"<p>Here you can select the number of CPUs you want to associate with your JupyterLab instance. The available choices are: 1, 4, 8, and 16. If you select a number of CPUs greater than 1, ensure that your code is able to use a multicore architecture; otherwise, your application will use only one CPU.</p> <p>If you don't know what to select, choose 1 CPU. You can always change it later.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users-k8s/#32-memory-ram","title":"3.2) Memory (RAM)","text":"<p>Here you can select the amount of RAM memory you want to associate with your JupyterLab instance. The available choices are: 4, 8, 16, and 32 GB. You should select the minimum value that allows your application to run.</p> <p>If you don't know what to select, choose 8 GB. You can always change it later.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users-k8s/#33-user-group","title":"3.3) User group","text":"<p>Users can have multiple groups, for example, because they collaborate with different teams (each with its own team directory).</p> <p>When users want their JupyterLab instance to be able to read/write files in these shared directories, the corresponding team group should be selected.</p> <p>If you don't know what to select, just leave the default value.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users-k8s/#34-gpu","title":"3.4) GPU","text":"<p>You will see this section only if you are allowed to use a GPU. Currently, only GPUs with a dedicated memory of 5GB are available. If your application requires a more powerful GPU, please open a ticket with the title \u201cReCaS HPC/GPU: JupyterHub with powerful GPU\u201d where you can explain the reasons you need a more powerful GPU. You should select a GPU only if your application is able to use it.</p> <p>The image shows that you can select different GPU models and how many of them are currently in use. You will only see GPU models you have been authorized to use.</p> <p>Only authorized users can select a GPU; please see below how to request one.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users-k8s/#35-additional-paths-to-mount","title":"3.5) Additional paths to mount","text":"<p>By default, your personal home directory (usually <code>/lustrehome/{username}</code>) is mounted AUTOMATICALLY inside your JupyterLab instance. DO NOT INSERT YOUR PERSONAL HOME DIRECTORY HERE.</p> <p>This configuration section allows you to mount additional and ALREADY CREATED directories, such as those used for sharing files in a team.</p> <p>Multiple paths can be inserted in this field, but they should be separated by commas with no spaces, e.g., <code>/lustre/path1,/lustre/path2</code>.</p> <p>Note</p> <p>These configuration fields are not permanent and can be changed. To do that, delete your instance and request a new one using the new configuration. Please see below how to delete your instance and create a new one.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users-k8s/#4-jupyterlab","title":"4) JupyterLab","text":"<p>Once the configuration phase is complete, the JupyterLab instance is launched. After its initialization, the following image shows the interface you will see.</p> <p></p> <p>There is a dedicated guide for JupyterLab, accessible at this link.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users-k8s/#5-deleting-your-jupyter-instance","title":"5) Deleting your Jupyter instance","text":"<p>The ReCaS datacenter does not have infinite resources.</p> <p>We appreciate it if you delete your Jupyter instance when you know you won't be using it for a while. When you need to use it, you can request a new one with a couple of clicks! You can also remove it if you want to change the instance configuration, for example, if you want to add more resources to it.</p> <p>To remove your Jupyter instance, access <code>Files -&gt; Hub Control Panel</code> as shown in the following image:</p> <p></p> <p>And then click the <code>Stop My Server</code> red button.</p> <p></p> <p>After a few seconds, your instance will be deleted. To request a new one, click the <code>Start My Server</code> blue button.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users-k8s/#6-additional-information","title":"6) Additional information","text":"<p>Each Jupyter instance is configured to be deleted automatically if you don't use it for at least a day.</p> <p>Please save all important files in your GPFS directories.</p> <p>Your Jupyter instance runs inside a container, and ONLY the files stored in directories in the ReCaS-Bari GPFS file system will be preserved, namely <code>/lustrehome/{username}</code> or <code>/lustre/{some-directory}</code>. Any local file content or module installations are lost if the container is stopped or crashes. Therefore, use the GPFS file system for all your important files.</p> <p>Containers could crash at any time, even during code execution. Partial results CANNOT be restored and will be lost. To manage this situation, consider storing all partial results in your HOME directory in GPFS.</p> <p>Also, consider that you do not have infinite space in the GPFS file system. Use it wisely.</p> <p>Deleting files using JupyterLab will create a hidden folder named .Trash in your HOME directory. To completely remove files, please access <code>frontend.recas.ba.infn.it</code> using SSH. The content of the .Trash folder contributes to your quota.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users-k8s/#7-service-request","title":"7) Service request","text":"<p>JupyterLab on the HPC/GPU cluster is available only to users with an active ReCaS-Bari HPC/HTC account and JupyterHub service enabled. Both can be requested via this link.</p> <p>In the following image, you can see how to request the ReCaS account, access to JupyterHub, and access to the GPU.</p> <p></p> <p>Access to JupyterHub is for everyone with a ReCaS account.</p> <p>The use of GPUs is restricted only to those with a STRONG motivation.</p> <p>You can check if the registration is successfully completed by connecting to the <code>frontend.recas.ba.infn.it</code> server via ssh:</p> <p><code>ssh &lt;username&gt;@frontend.recas.ba.infn.it</code></p> <p>After that, you can request your personal JupyterLab instance via this link.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users/","title":"JupyterHub for ReCaS users","text":"<p>Updated on 26Jul2023</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users/#important-user-support","title":"IMPORTANT: User Support","text":"<p>If you need support for your application, please use this link to create a ticket with title \u201cReCaS HPC/GPU: Jupyter support\u201d and then describe your issue.</p> <p>It is STRONGLY advised to subscribe to the recas-hpu-gpu mailing list. Create a ticket with the title \u201cReCaS HPC/GPU: subscribe to the mailing list\u201d.</p> <p>Important messages will be sent ONLY using the mailing list.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users/#1-introduction","title":"1) Introduction","text":"<p>The ReCaS JupyterHub provides Jupyter Lab instances for the ReCaS users. Jupyter Labs are open-source web applications that allows you to create and share documents that contain live code, equations, visualisations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modelling, data visualisation, machine learning, and much more.</p> <p>Jupyter Lab served through this service can access your files and directories stored in the ReCaS-Bari storage (based on GPFS distributed file system) and use high performance GPUs to speed up the execution of your application. </p> <p>Only registered users can access to this service. In the request section you can find information how to request the service.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users/#2-access-to-the-service","title":"2) Access to the service","text":"<p>The ReCaS JupyterHub service provides a JupyterLab instance:</p> <ul> <li> <p>with GPU, accessible at this link</p> </li> <li> <p>with NO GPU, accessible at this link</p> </li> </ul> <p>JupyterLab instances with NO GPU are available for those with a ReCaS account.</p> <p>Users willing to use a JupyterLab instances with GPU, should provide a STRONG motivation.</p> <p>Once you have clicked on the link, the authentication page will be shown:</p> <p></p> <p>Here you should insert your personal username and password, created during the registration phase.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users/#3-configuration-section","title":"3) Configuration section","text":"<p>After authenticated, you will have the possibility to configure your jupyterLab instance. The following image shows the configuration page.</p> <p></p> <p>As you can see, you have the possibility to configure 5 fields.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users/#31-additional-paths-to-mount","title":"3.1) Additional paths to mount","text":"<p>By default your personal home directory (usually <code>/lustrehome/{username}</code>) is mounted inside your Jupyter Lab instance DO NOT INSERT YOUR PERSONAL HOME DIRECTORY HERE. </p> <p>This configuration section allows you to mount additional and ALREADY CREATED directories, like those used to shared files in a team. </p> <p>Multiple paths can be inserted in this field but they should be separeted using the comma without spaces, e.g. <code>/lustre/path1,/lustre/path2</code>. </p>"},{"location":"interactive_services/jupyter-lab-for-recas-users/#32-user-group","title":"3.2) User group","text":"<p>Users can have multiple groups, for example because collaborate with different teams (each having own team directory).</p> <p>When users want their Jupyter Lab instance can read/write files in this shared directories, the corresponding team group should be selected.</p> <p>If you don't know what select, just leave the default value.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users/#33-cpus","title":"3.3) CPUs","text":"<p>Here you can select the number of cpus you want associate with your Jupyter Lab instance.  The available choices are: 1, 2, 4, 8 and 16. If you select a number of cpus greater than 1 ensure that your code is able to use a multicore architecture, otherwise your application will use only 1 cpu. </p> <p>If you don't know what select, select 1 cpu. You have always to change it later. </p>"},{"location":"interactive_services/jupyter-lab-for-recas-users/#34-memory-ram","title":"3.4) Memory (RAM)","text":"<p>Here you can select the amount of RAM memory you want associate with your Jupyter Lab instance.  The available choices are: 8, 16 and 32 GB. You should select the mininum value that allow your application to run.</p> <p>If you don't know what select, select 8 GB. You have always to change it later. </p>"},{"location":"interactive_services/jupyter-lab-for-recas-users/#35-gpu","title":"3.5) GPU","text":"<p>You can select to add a GPU to your instance.  In this moment only GPU with a dedicated memory of 5GB are available. In the case your application requires a more powerful GPU, please open a ticket with the title \u201cReCaS HPC/GPU: Jupyter with powerful GPU\u201d where you can explain the reasons you need a more powerful GPU. You should select the GPU only if your application is able to use it. </p> <p>Only authorized users can select a GPU, please see below how to request one.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users/#36-change-configurations","title":"3.6) Change configurations","text":"<p>These configuration fields are not permanent and can be changed. To do that, delete your instance and request a new one using the new configuration. Please, see below how to delete your instance and create a new one.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users/#4-jupyterlab","title":"4) JupyterLab","text":"<p>As soon as the configuration phase is terminated, the Jupyter Lab instance is launched.  After its initialization, the following image shows you the interface you will see.</p> <p></p> <p>There is a dedicated guide for Jupyter Lab, accessible to this link.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users/#5-delete-you-jupyter-instance","title":"5) Delete you Jupyter instance","text":"<p>**The ReCaS datacenter does not have infinite resources. **</p> <p>We apprecciate if you decide to delete your Jupyter instance in the case you know in the near future you will not use it.  When you will need to use it, you can request a new one with a couple of clicks! You could remove it also in the case you would like to change the instance configuration, for example is you want add more resources to it.</p> <p>To remove your Jupyter instance, access to <code>Files -&gt; Hub Control Panel</code> as shown in the following image:</p> <p></p> <p>And then click on the `Stop My Server' red button.</p> <p></p> <p>After few seconds, your instance will be deleted. If you would request a new one, click on the <code>Start My Server</code> blue button.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users/#6-additional-informations","title":"6) Additional informations","text":"<p>Each Jupyter instance is configured to delete it automatically if you don't use it for at least a day. </p> <p>Please save all important files in your GPFS directories.</p> <p>Your Jupyter instance is executed inside a container and ONLY the files stored in directories in the ReCaS-Bari GPFS file system will be preserved, namely <code>/lustrehome/{username}</code> or <code>/lustre/{some-directory}</code>. Any local file content or module installation are lost if the container is stopped or crashes. So, use the GPFS file system for all your important files.</p> <p>Containers could crash at any time also during the execution of the code, partial results CAN NOT be restored and will be lost. To manage this situation, consider storing in your HOME directory in GPFS all partial results.</p> <p>Also consider that you do not have infinite space in the GPFS file system. Use it wisely.</p> <p>Delete files using Jupyter Lab will create a .Trash hidden folder in your HOME directory. To remove completely files, please access using SSH to <code>frontend.recas.ba.infn.it</code>. The content of the .Trash folder contribute to your quota.</p>"},{"location":"interactive_services/jupyter-lab-for-recas-users/#7-service-request","title":"7) Service request","text":"<p>Jupyter Lab on HPC/GPU cluster is available only for those users with a ReCaS-Bari HPC/HTC account active and the access to the JupyterHub service enabled. Both of them can be request using this link.</p> <p>In the following image you can see how to request the ReCaS account, the access to the JupyterHub and the access to the GPU.</p> <p></p> <p>The access to JupyterHub is for everyone with a ReCaS Account.</p> <p>The use of GPUs is restricted only for those having a STRONG motivation.</p> <p>You can check if the registration is successfully completed by access to the <code>frontend.recas.ba.infn.it</code> server via ssh:</p> <p><code>ssh &lt;username&gt;@frontend.recas.ba.infn.it</code></p> <p>After that, you can request your personal Jupyter Lab instance using this link.</p>"},{"location":"interactive_services/jupyter_notebook/","title":"Jupyter Notebook","text":"<p>Updated on 04Oct2022</p>"},{"location":"interactive_services/jupyter_notebook/#0-user-support","title":"0 User Support","text":"<p>If you need support for your application, please use this link to create a ticket with title \u201cReCaS HPC/GPU: Jupyter Notebook support\u201d and then describe your issue.</p> <p>It is STRONGLY advised to subscribe to the recas-hpu-gpu mailing list. Create a ticket with the title \u201cReCaS HPC/GPU: subscribe to the mailing list\u201d.</p>"},{"location":"interactive_services/jupyter_notebook/#1-introduction","title":"1 Introduction","text":"<p>The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualisations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modelling, data visualisation, machine learning, and much more. Through Jupyter Notebook, you can access your files and directories stored in the ReCaS-Bari GPFS file system and browse graphically, as shown in the following figure.</p> <p></p> <p>To create a new notebook with Python 3, click on \u201cNew\u201d and then select \u201cPython 3\u201d, as shown in the following figure.</p> <p></p> <p>Finally, the Integrated Development Environment (IDE) is opened, as shown in the figure.</p> <p></p> <p>The Jupyter Notebook instance MUST BE used ONLY for the developing phase. As soon as the code/analysis works, users are encouraged to use the Job Orchestration service.</p> <p>Note</p> <p>Every Jupyter Notebook instance on HPC/GPU cluster is allowed ONLY for a month. After, the instance will be killed. An email will be sent a week before. To extend the period of usage, create a ticket using this link with title \u201cReCaS HPC/GPU: Jupyter Notebook instance - Time extension request\u201d and describe a valid reason for extension of time.</p>"},{"location":"interactive_services/jupyter_notebook/#2-service-request","title":"2 Service request","text":"<p>Jupyter Notebook on HPC/GPU cluster is available only for those users with a ReCaS-Bari HPC/HTC account active. Users without such an account MUST register using this link (check the box \"Account for access to ReCas-Bari compute services (HTC/HPC)\").</p> <p>You can check if the registration is successfully completed by access to the <code>frontend.recas.ba.infn.it</code> server via ssh:</p> <p><code>ssh &lt;username&gt;@frontend.recas.ba.infn.it</code></p> <p>After that, you can request your personal Jupyter Notebook instance using this link.</p> <p>Please provide the following information:</p> <pre><code>Title: \u201cReCaS HPC/GPU: Jupyter Notebook instance request\u201d\nIssue:\n- Name and Surname\n- Username\n- Email\n- number of required CPU\n- number of required GPU\n- amount of RAM\n- hashed password\n- other info, if you believe could be useful (like python version)\n</code></pre> <p>Note</p> <p>Please identify only the resource you need (we don't have infinite resources for all users!)</p> <p>Once the request will be approved, you will receive an email containing your URL to use to access the remote Jupyter Notebook.</p> <p>In the following image, you can see the login web page. Insert your password used during the creation of hashed password (Next section).</p> <p></p> <p>IMPORTANT</p> <p>DO NOT SHARE your password. People knowning hostname, port and password have access to your home directory with delete permission.</p>"},{"location":"interactive_services/jupyter_notebook/#21-preparing-a-hashed-password","title":"2.1 Preparing a hashed password","text":"<p>You can prepare a hashed password manually.</p> <p>Open a shell, install the notebook python module (pip3 install notebook) and type the following lines in a python shell:</p> <pre><code>[root@your-machine ~]# python3\n&gt;&gt;&gt; from jupyter_server.auth import passwd\n&gt;&gt;&gt; passwd()\nEnter password:\nVerify password:\n'argon2:$argon2id$v=19$m=10240,t=10,p=8$+Gzqn+ZgyvjrXo9eJTIe3w$z0fzG6RZgSbcSXkCAYb3vw'\n</code></pre> <p>Finally, save the string <code>'argon2:$argon2id$v=19$m=10240,t=10,p=8$+Gzqn+ZgyvjrXo9eJTIe3w$z0fzG6RZgSbcSXkCAYb3vw'</code></p> <p>And provide it among the required information.</p> <p>For reference, the official web page.</p>"},{"location":"interactive_services/jupyter_notebook/#3-important-information","title":"3 Important information","text":"<p>Your instance of Jupyter notebook is executed inside a container and ONLY the files stored in your HOME directory in the ReCaS-Bari GPFS file system will be preserved if faults occur, namely /lustrehome. Any local file content or module installation are lost if the container is stopped or crashes. So, use the GPFS file system for all your important files.</p> <p>Containers could crash at any time also during the execution of the code, partial results CAN NOT be restored and will be lost. To manage this situation, consider storing in your HOME directory in GPFS all partial results.</p> <p>Also consider that you do not have infinite space in the GPFS file system. Use it wisely.</p> <p>Delete files using Jupyter Notebook will create a .Trash hidden folder in your HOME directory. To remove completely files, please access using SSH to <code>frontend.recas.ba.infn.it</code>. The content of the .Trash folder contribute to your quota.</p>"},{"location":"interactive_services/jupyter_notebook/#4-notebook-tips","title":"4 Notebook tips","text":"<p>To install python modules directly inside the code is needed to add some lines at the beginning.</p> <p>Following lines install multiple modules inside the jupyter notebook and then import all of them. Replace <code>&lt;user&gt;</code> with your username.</p> <pre><code>import sys\nif not '/home/&lt;user&gt;/.local/lib/python3.8/site-packages' in sys.path:\n`   `sys.path.append('/home/&lt;user&gt;/.local/lib/python3.8/site-packages')\n\n!{sys.executable} -m pip install opencv-python-headless\n!{sys.executable} -m pip install seaborn\n!{sys.executable} -m pip install keras\n!{sys.executable} -m pip install scipy\n!{sys.executable} -m pip install pandas\n!{sys.executable} -m pip install scikit-learn\n!{sys.executable} -m pip install matplotlib\nimport cv2\nimport sql\nimport seaborn\nimport keras\nimport scipy\nimport pandas\nimport sklearn\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"interactive_services/jupyter_with_gpu/","title":"ReCaS JupyterHub with more Resources","text":"<p>Updated on 03Jul2025</p>"},{"location":"interactive_services/jupyter_with_gpu/#0-user-support","title":"0 User Support","text":"<p>If you need support for your application, please use this link to create a ticket with title \u201cReCaS HPC/GPU: Jupyter Lab support\u201d and then describe your issue.</p> <p>It is STRONGLY advised to subscribe to the recas-hpu-gpu mailing list. Create a ticket with the title \u201cReCaS HPC/GPU: subscribe to the mailing list\u201d.</p>"},{"location":"interactive_services/jupyter_with_gpu/#1-introduction","title":"1 Introduction","text":"<p>When requesting GPU resources in the ReCaS JupyterHub service, users are provisioned with a modestly-powered virtual GPU. The rationale behind this is twofold: firstly, the majority of analytical workloads do not necessitate dedicated access to an entire GPU. Secondly, for applications requiring substantial computational power, this virtual GPU provides an environment to prototype and validate code against smaller datasets. Subsequent execution can then be offloaded to higher-performance GPUs using the Job Orchestration service. There are cases that needs more computation resources during the code development. If this is your case, you are in the right place. </p>"},{"location":"interactive_services/jupyter_with_gpu/#2-service-request","title":"2 Service request","text":"<p>More compuration resources are available only for those users with a ReCaS-Bari HPC/HTC account active. Users without such an account MUST register using this link (check the box \"Account for access to ReCas-Bari compute services (HTC/HPC)\").</p> <p>You can check if the registration is successfully completed by access to the <code>frontend.recas.ba.infn.it</code> server via ssh:</p> <p><code>ssh &lt;username&gt;@frontend.recas.ba.infn.it</code></p> <p>After that, you can request GPUs using this link.</p> <p>Please provide the following information:</p> <pre><code>Title: \u201cReCaS HPC/GPU: Jupyter Lab instance request\u201d\nIssue:\n- Name and Surname\n- Username\n- Email\n- number of required CPU\n- number and GPU model\n- amount of RAM\n- other info, if you believe could be useful (like python version)\n</code></pre> <p>Note</p> <p>Please identify only the resource you need (we don't have infinite resources for all users!)</p> <p>After your request is accepted, a confirmation email will arrive. Then, just access the JupyterHub service, and your requested GPU will be listed among the usable resources.</p>"},{"location":"interactive_services/jupyter_with_gpu/#3-important-information","title":"3 Important information","text":"<p>Your instance of JupyterLab is executed inside a container and ONLY the files stored in your HOME directory in the ReCaS-Bari GPFS file system will be preserved if faults occur, namely /lustrehome. Any local file content or module installation are lost if the container is stopped or crashes. So, use the GPFS file system for all your important files.</p> <p>Containers could crash at any time also during the execution of the code, partial results CAN NOT be restored and will be lost. To manage this situation, consider storing in your HOME directory in GPFS all partial results.</p> <p>Also consider that you do not have infinite space in the GPFS file system. Use it wisely.</p> <p>Delete files using Jupyter Lab will create a .Trash hidden folder in your HOME directory. To remove completely files, please access using SSH to <code>frontend.recas.ba.infn.it</code>. The content of the .Trash folder contribute to your quota.</p>"},{"location":"interactive_services/rstudio/","title":"RStudio","text":"<p>Updated on 19Apr2022 </p>"},{"location":"interactive_services/rstudio/#1-introduction","title":"1 Introduction","text":"<p>RStudio is an Integrated Development Environment (IDE) for R, a programming language for statistical computing and graphics. ReCaS-Bari provides RStudio Server that runs on a remote server and allows accessing RStudio using any web browser.</p> <p>The following figure shows the provided IDE you can access using your web browser.</p> <p></p> <p>In the bottom right section you are able to browse graphically your HOME directory located in the ReCaS-Bari GPFS file system.</p>"},{"location":"interactive_services/rstudio/#2-service-request","title":"2 Service Request","text":"<p>Rstudio on HPC/GPU cluster is available only for those users with a ReCaS-Bari HPC/HTC account active. Users without such an account MUST register using this link (check the box \"Account for access to ReCas-Bari compute services (HTC/HPC)\").</p> <p>You can check if the registration is successfully completed by access to the <code>frontend.recas.ba.infn.it</code> server via ssh:</p> <p><code>ssh &lt;username&gt;@frontend.recas.ba.infn.it</code></p> <p>After that, you can request your personal RStudio instance using this link.</p>"},{"location":"interactive_services/rstudio/#21-gpu-request","title":"2.1 GPU request","text":"<p>GPUs are a valuable resource, so we only grant them when strictly necessary and when there is ready-to-use code that runs on CPUs and only needs to be adapted and executed on GPUs.</p> <p>Only one GPU can be requested at a time.  The request must provide a justification for using a GPU and explain why CPUs are not sufficient.</p> <p>If the GPU request is approved, it will be granted for one month only.  At the end of this period, the user must send an additional request (by email) to extend the service usage for another month.  If this request is not approved or if the user does not send an email, the instance will be terminated without notice.</p>"},{"location":"interactive_services/rstudio/#22-request-format","title":"2.2 Request format","text":"<p>Please provide the following information:</p> <pre><code>Title: \u201cReCaS HPC/GPU: RStudio instance request\u201d\nIssue:\n- Name and Surname\n- Username\n- Email\n- number of required CPU: allowed values [1, 2, 4, 8, 16]\n- GPU: allowed values [0, 1]\n- amount of RAM: allowed values [4GB, 8GB, 16GB, 32GB, 64GB]\n- other info, if you believe could be useful\n</code></pre> <p>After that you will receive an email to the same address containing your URL to use to access the remote Rstudio Server.</p> <p>The following image shows you the RStudio login web page. Use your ReCaS username and password to access.</p> <p></p>"},{"location":"interactive_services/rstudio/#3-important-information","title":"3 Important information","text":"<ul> <li> <p>Your instance of RStudio is executed inside a container and ONLY the files stored in your HOME directory in the ReCaS-Bari GPFS file system will be preserved if faults occur, namely /lustrehome. Any local file content or module installation is lost if the container is stopped or crashes. So, use the GPFS file system for all your important files.</p> </li> <li> <p>Containers could crash at any time also during the execution of the code, partial results CAN NOT be restored and will be lost. To manage this situation, consider storing in your HOME directory in GPFS all partial results.</p> </li> <li> <p>Also consider that you do not have infinite space in the GPFS file system. Use it wisely.</p> </li> </ul>"},{"location":"interactive_services/rstudio/#4-user-support","title":"4 User Support","text":"<p>If you need support for your application, please use this link to create a ticket with title \u201cReCaS HPC/GPU: RStudio support\u201d and then describe your issue.</p> <p>Tip</p> <p>It is STRONGLY advised to subscribe to the recas-hpu-gpu mailing list. Create a ticket with the title \u201cReCaS HPC/GPU: subscribe to the mailing list\u201d.</p>"},{"location":"job_submission/chronos/","title":"Job orchestrator with Chronos","text":"<p>Updated on 04Oct2022</p>"},{"location":"job_submission/chronos/#0-user-support","title":"0 User Support","text":"<p>If you need support for your application, please use this link to create a ticket with title \u201cReCaS HPC/GPU: Chronos instance support\u201d and then describe your issue.</p> <p>It is STRONGLY advised to subscribe to the recas-hpu-gpu mailing list. Create a ticket with the title \u201cReCaS HPC/GPU: subscribe to the mailing list\u201d.</p>"},{"location":"job_submission/chronos/#1-introduction","title":"1 Introduction","text":"<p>Chronos is the tool used to submit jobs to the ReCaS-Bari HPC/GPU cluster.</p> <p>Note</p> <p>ONLY Docker containers can be executed in the cluster.</p> <p>You can run a third party Docker container or you can build your custom one. Please refer to the guide at this link for more details.</p>"},{"location":"job_submission/chronos/#2-request-your-personal-recas-chronos-instance","title":"2 Request your personal ReCaS Chronos instance","text":"<p>Chronos is available only for users with a ReCaS-Bari HPC/HTC account active. Users without such an account MUST register using this link (check the box \"Account for access to ReCas-Bari compute services (HTC/HPC)\").</p> <p>You can verify if the registration is successfully completed by access to the frontend.recas.ba.infn.it server via ssh:</p> <pre><code>   ssh &lt;your-username&gt;@frontend.recas.ba.infn.it\n</code></pre> <p>After that, you can request your personal Chronos instance using this link.</p> <p>Please provide the following information:</p> <pre><code>   Title: \u201cReCaS HPC/GPU: new Chronos instance request\u201d\n   Issue:\n   - Name and Surname\n   - Username\n   - Email\n</code></pre>"},{"location":"job_submission/chronos/#3-chronos-web-interface","title":"3 Chronos Web Interface","text":"<p>Chronos provdes a simple and fast web interface as shown in the following figure.</p> <p></p> <p>Through the web interface is possible to verify your job status. Moreover, it can be used to submit scheduled or dependent jobs.</p>"},{"location":"job_submission/chronos/#31-submit-a-job-using-the-web-interface","title":"3.1 Submit a job using the Web Interface","text":"<p>The web interface provides the possibility to submit a job by clicking on the \u201cADD JOB\u201d. The following figure shows the box where you can insert all job parameters.</p> <p></p> <p>After adding the job, if the requested resources are available, the job is started. Otherwise the job is put in the queued state.</p> <p>The following image shows the status of a job.</p> <p></p> <p>The boxes on the right allow to start again the job, edit the job description, stop and delete a job, respectively.</p>"},{"location":"job_submission/chronos/#32-submit-the-job-using-the-terminal","title":"3.2 Submit the job using the terminal","text":"<p>Although submitting jobs through the web interface might seem easier at first, it could be not comfortable, scalable and fast. Submitting jobs using the terminal is RECOMMENDED.</p> <p>This method requires a JSON file containing all the job information and the execution of a command that transmits the job information to your personal Chronos instance. Following an example of a JSON file. Insert the information of your specific job and save the file.</p> <pre><code>{\n    \"name\": \"&lt;detailed-and-unique-job-name&gt;\",\n    \"command\": \"python3.6 &lt;/lustre/path/to/your/code&gt;\",\n    \"shell\": true,\n    \"retries\": 2,\n    \"description\": \"\",\n    \"cpus\": 4,\n    \"disk\": 10,\n    \"mem\": 8192,\n    \"gpus\": 1,\n    \"environmentVariables\": [],\n    \"arguments\": [],\n    \"runAsUser\": \"&lt;your-username&gt;\",\n    \"owner\": \"&lt;your-username&gt;\",\n    \"ownerName\": \"&lt;your-username&gt;\",\n    \"container\": {\n        \"forcePullImage\": true,\n        \"type\": \"mesos\",\n        \"image\": \"&lt;your-container-image&gt;\",\n        \"volumes\": [{\"containerPath\": \"/lustre/path/to/your/home-directory&gt;\", \n                     \"hostPath\": \"/lustre/path/to/your/home-directory\", \n                     \"mode\": \"RW\"}]\n    },\n    \"schedule\": \"R1//P1Y\"\n}\n</code></pre> <p>Note</p> <ul> <li>It\u2019s important to insert an unique name for each submitted job, this will be important during the debugging phase. The name should ALWAYS start with your username. Insert dash ( - ) between words. Es \u201cmyuser-test-job-1-date-2020-09-34\u201d.</li> <li>The command field is specific for each application, what written in the example will not work in your use case. Absolute path is preferred.</li> <li>&lt;your-username&gt; is the user created during the registration.</li> <li>&lt;your-container-image&gt; is the container image name to use for your application.</li> <li>The schedule field is used to type how many times you want to execute the job and the period among execution, please refer to this guide (Adding a Scheduled Job - section) if you need it otherwise leave it as you see in the example.</li> </ul> <p>The command used to submit the job is based on CURL. On a machine with a Linux OS, open a file editor and copy the following lines inside and save it (e.g. with the \u201csubmit-to-chronos\u201d name).</p> <pre><code>   #!/bin/bash\n   FILE=$1\n   USERNAME=&lt;your-username&gt;\n   PASSWORD=&lt;chronos-instance-password&gt;\n   HOSTNAME=&lt;chronos-instance-hostname&gt;\n   PORT=&lt;chronos-instance-port&gt;\n   curl -u $USERNAME:$PASSWORD -L -H 'Content-Type: application/json' -X POST --data-binary \"@$FILE\" http://$HOSTNAME:$PORT/v1/scheduler/iso8601\n</code></pre> <p>Once the request will be approved, the administrator will provide &lt;chronos-instance-password&gt;, &lt;chronos-instance-hostname&gt; and &lt;chronos-instance-port&gt; by email.</p> <p>To submit the job, execute the following command (supposing the json file name is \u201cjob.json\u201d and the submit command file name is \u201csubmit_chronos\u201d)</p> <pre><code>   bash ./submit_chronos job.json\n</code></pre> <p>Note</p> <p>Since your Chronos instance is not accessible from outside, the unique procedure to submit job is to store json files and the script used to submit jobs in your home directory on frontend.recas.ba.infn.it (officially the ReCaS-Bari storage based on GPFS) and execute the command from there.</p>"},{"location":"job_submission/chronos/#33-user-support","title":"3.3 User Support","text":"<p>For any problem related to the Chronos service, use this link for create a support request, inserting as title \u201cReCaS HPC/GPU: Chronos issue\u201d, then describe the problem in the issue box. Your username and job_name MUST be added.</p> <p>At the moment, users can not access to the job logs. For support on a specific job, submit a support request providing the username, job_name and describing the problem in the issue box.</p> <p>Tip</p> <p>It is STRONGLY advised to subscribe to the recas-hpu-gpu mailing list. Create a ticket with the title \u201cReCaS HPC/GPU: subscribe to the mailing list\u201d.</p>"},{"location":"job_submission/chronos/#34-access-to-the-chronos-web-interface","title":"3.4 Access to the Chronos Web Interface","text":"<p>In order to access your Chronos instance is needed to create a SSH Tunnel.  </p> <p>Open a shell and execute the following command: <pre><code>   ssh -f -N -L 127.0.0.1:&lt;localhost-port&gt;:&lt;chronos-instance-hostname&gt;:&lt;chronos-instance-port&gt; &lt;your-username&gt;@frontend.recas.ba.infn.it\n</code></pre></p> <p>Note</p> <p>&lt;localhost-port&gt;: you can choose the port by yourself</p> <p>IMPORTANT</p> <p>The SSH tunnel works in background (-f flag), if the process crashes or you reboot your machine, you have to create again the SSH tunnel using the above command</p> <p>Now you are able to access to your Chronos instance using your favorite browser and the url \"localhost:&lt;localhost-port&gt;\" </p>"},{"location":"job_submission/chronos/#4-example","title":"4 Example","text":"<p>The following JSON could be used as first test by the user.</p> <pre><code>{\n  \"name\": \"&lt;username&gt;-my-first-job-submission\",\n  \"command\": \"sleep 5 &amp;&amp; nvidia-smi\",\n  \"shell\": true,\n  \"retries\": 4,\n  \"description\": \"\",\n  \"cpus\": 1,\n  \"disk\": 0,\n  \"mem\": 1024,\n  \"gpus\": 1,\n  \"environmentVariables\": [],\n  \"arguments\": [],\n  \"runAsUser\": \"&lt;username&gt;\",\n  \"owner\": \"&lt;username&gt;\",\n  \"ownerName\": \"&lt;username&gt;\",\n  \"container\": {\n    \"forcePullImage\": true,\n    \"type\": \"mesos\",\n    \"image\": \"registry-clustergpu.recas.ba.infn.it/gvino/cuda11.5.0-base-ubuntu20.04:0.1\"\n  },\n  \"schedule\": \"R1//P1Y\"\n}\n</code></pre> <p>Save it as \"my-first-job.json\" on your personal computer and send this file and that used to submit the job to your home in the ReCaS-Bari Storage using the command:</p> <pre><code>scp ./my-first-job.json submit_chronos &lt;username&gt;@frontend.recas.ba.infn.it:\n</code></pre> <p>Then access to your storage using:</p> <pre><code>ssh &lt;username&gt;@frontend.recas.ba.infn.it:\n</code></pre> <p>Finally, submit the job using the command:</p> <pre><code>bash submit_chronos my-first-job.json\n</code></pre> <p>Go to the web inteface to verify whether the job was successfully submitted and executed.</p>"},{"location":"job_submission/htcondor/","title":"Job orchestrator with HTCondor","text":"<p>Updated on 07Jul2025</p>"},{"location":"job_submission/htcondor/#0-user-support","title":"0 User Support","text":"<p>If you need support for your application, please use this link to create a ticket with title \u201cReCaS HPC/GPU: Chronos instance support\u201d and then describe your issue.</p>"},{"location":"job_submission/htcondor/#1-official-htcondor-guides","title":"1 Official HTCondor guides","text":"<p>Italian version</p> <p>English version</p>"},{"location":"job_submission/k8s-jobs/","title":"Job submission using Kubernetes","text":"<p>Updated on 15Jul2025</p>"},{"location":"job_submission/k8s-jobs/#important-user-support","title":"IMPORTANT: User Support","text":"<p>If you need support, please use this link to submit a ticket with title \u201cReCaS HPC/GPU: Kubernetes support\u201d and then describe your issue.</p> <p>It is STRONGLY advised to subscribe to the recas-hpc-gpu mailing list. Create a ticket with the title \u201cReCaS HPC/GPU: subscribe to the mailing list\u201d.</p> <p>Important messages will be sent ONLY using the mailing list.</p>"},{"location":"job_submission/k8s-jobs/#1-introduction","title":"1) Introduction","text":"<p>Kubernetes (K8s) is the tool used to submit jobs to the ReCaS-Bari HPC/GPU cluster.</p> <p>Note ONLY containers can be executed in the cluster.</p> <p>You can run a third party container or you can build your custom one. Please refer to the guide at this link for more details on the latter.</p>"},{"location":"job_submission/k8s-jobs/#2-access-to-the-service","title":"2) Access to the service","text":"<p>Access to our HPC/GPU Kubernetes Cluster is available only for users with a ReCaS-Bari HPC/HTC account active. Users without such an account MUST register using this link (check the box \"Account for access to ReCas-Bari compute services (HTC/HPC)\"). Once your request has been submitted, it must be approved by the service manager. This process usually takes a few working days. You will receive an automatic email notification as soon as your account is activated.</p> <p>Once activated, you can verify if the registration is successfully completed by accessing the frontend.recas.ba.infn.it server via ssh:</p> <p><code>ssh &lt;your-username&gt;@frontend.recas.ba.infn.it</code></p> <p>After that, you can request access to the Kubernetes service using this link.</p> <p>Please provide the following information:</p> <pre><code>   Title: \u201cReCaS HPC/GPU: request to access HPC/GPU K8s cluster\u201d  \n   Issue:\n   - Name and Surname\n   - HTC/HPC service username\n   - Email  \n</code></pre>"},{"location":"job_submission/k8s-jobs/#3-configuring-access-to-the-kubernetes-cluster","title":"3) Configuring access to the Kubernetes cluster","text":"<p>This section explains how to set up the kubectl command-line tool to access the ReCaS HPC/GPU Kubernetes cluster after your access request has been approved\u2014that is, once you\u2019ve received a positive response to the ticket titled \"ReCaS HPC/GPU: request to access HPC/GPU K8s cluster\" as described in Section 2.</p>"},{"location":"job_submission/k8s-jobs/#31-kubectl","title":"3.1) kubectl","text":"<p>Interaction with a Kubernetes (K8s) cluster happens through an API server. Any action you perform \u2014 whether monitoring resources, deploying workloads or checking logs \u2014 ultimately goes through this server, which listens for HTTPS requests.</p> <p>While you could technically interact with the API server using tools like curl, it's far easier and more efficient to use a dedicated CLI tool: kubectl. kubectl is the official Kubernetes command line client, designed to communicate with the cluster's API server in a user-friendly way. It offers commands for inspecting resources, deploying applications, scaling workloads and more.</p> <p>The tool is already installed on the ReCaS frontend (frontend.recas.ba.infn.it) and can also be easily installed on your local machine if needed. Please note that, in case of local installation, you should use version 1.30.0 to ensure compatibility. Using a different version may lead to issues.</p> <p>To configure kubectl for access to the ReCaS Kubernetes cluster \u2014 either from the frontend or from your own system \u2014 you\u2019ll need to create a configuration file with the correct cluster and authentication details.  </p> <p>kubectl is configured to look for its configuration file at the path .kube/config within the home directory of the user executing the command. To set this up, begin by creating the .kube directory:</p> <p><code>mkdir ~/.kube</code></p> <p>Then, using your preferred text editor (vim, nano, emacs, etc.),  create the ~/.kube/config file and paste the following template into it:</p> <pre><code>apiVersion: v1\nkind: Config\npreferences: {}\nclusters:\n- name: default\n  cluster:\n    server: https://k8s.recas.ba.infn.it:443\n\ncontexts:\n- name: oidc-user-context\n  context:\n    cluster: default\n    user: oidc-user\n    namespace: batch-&lt;your-frontend-username&gt;\n\ncurrent-context: oidc-user-context\n\nusers:\n- name: oidc-user\n  user:\n    token: \n</code></pre> <p>The only part that needs to be edited in this manifest is the section contexts.name.context.namespace. Replace &lt;your-frontend-username&gt; with your actual cluster username. So, for example, if you connect to the cluster with  </p> <p><code>ssh fdebiase@frontend.recas.ba.infn.it</code> </p> <p>then the namespace should be:  </p> <p><code>namespace: batch-fdebiase</code></p> <p>Once you\u2019ve made these changes and saved the file, kubectl will automatically use this configuration by default whenever you run commands.</p>"},{"location":"job_submission/k8s-jobs/#32-access-token","title":"3.2) Access token","text":"<p>As you probably noticed, the last field of the text file you edited is an empty field called token. This field needs to be filled with a personal access token (of weekly expiration), which is required for authentication. </p>"},{"location":"job_submission/k8s-jobs/#requesting-the-token","title":"Requesting the token","text":"<p>In order to get this token, you have to login using your frontend (HTC/HPC service) credentials at https://auth-k8s.recas.ba.infn.it</p> <p>After the authentication procedure is complete, you will see on your browser page (see image below) a token string: it is an encoded object (JWT) with all the information Kubernetes needs to know in order to authenticate you whenever you try to make a request to the API server(s) using the kubectl tool.</p> <p></p>"},{"location":"job_submission/k8s-jobs/#configure-kubectl-to-use-the-token","title":"Configure kubectl to use the token","text":"<p>Now that you have your token, update the token: field in the users section of your kubectl config file.  </p> <p>So, supposing your token is 'JALPFNGBQLBVaaaQG' the configuration file has to look like:</p> <pre><code>users:\n- name: oidc-user\n  user:\n    token: JALPFNGBQLBVaaaQG\n</code></pre> <p>That\u2019s it! Your configuration is now complete.</p> <p>\u26a0\ufe0f IMPORTANT NOTE Your access token is strictly personal. You are fully responsible for any operations performed using your personal token. If you are using kubectl directly on the ReCaS HTCondor cluster frontend, ensure that your Kubernetes configuration file (which contains your token) has restrictive permissions\u2014accessible only by you. To enforce this, simply run the following command:  <code>chmod 700 ~/.kube/config</code> This sets the file permissions to allow read, write, and execute access only for your user.</p> <p>Note Tokens are valid for 7 days. When your token expires, kubectl commands will stop working with an error. To fix this, simply log in again at the URL above, retrieve a new token, and replace it in the config file. </p>"},{"location":"job_submission/k8s-jobs/#33-verifying-your-setup","title":"3.3) Verifying your setup","text":"<p>After your token is in place, for the next week (that is, until the token expires) you can directly interact with the Kubernetes cluster using kubectl. To check that everything is configured correctly, run:  </p> <p><code>kubectl get pod</code> </p> <p>If everything is working, you\u2019ll see:  </p> <p><code>No resources found in batch-{yourUsername} namespace</code> </p> <p>This means your token is valid and kubectl can communicate with the cluster.  Otherwise, if you get an error like:  </p> <p><code>Error from server (Forbidden): pods is forbidden: User \"{yourUsername}\" cannot list resource \"pods\" in API group \"\" in the namespace \"batch-{yourUsername}\"</code> </p> <p>then there might be a misconfiguration. Double-check the steps above; if the issue persists, reach out to support.</p>"},{"location":"job_submission/k8s-jobs/#4-jobs-submission","title":"4) Jobs submission","text":"<p>This section explains how to submit a containerized workload as a Kubernetes Job to the ReCaS HPC/GPU cluster.</p>"},{"location":"job_submission/k8s-jobs/#41-jobs-limitation","title":"4.1) Jobs limitation","text":"<p>By default, users are subject to resource quotas, which limit the total amount of CPU cores, RAM and GPUs that can be requested. These limits apply across all your active jobs combined, not just to individual jobs. Currently, each user is limited to a maximum of 80 CPU cores, 300 GB of RAM and 2 GPUs across all active jobs. Additionally, each individual job can run for a maximum of one week (24*7 hours). If the job exceeds this runtime, it will be automatically terminated by the system. If you believe your workload requires an exception to any of these limits, please contact us and describe your use case.</p> <p>Note The cluster has a large \u2014 but still finite \u2014 pool of computing resources. If all available GPUs or other resources are already in use by other users, your job will not start immediately. Instead, Kubernetes will automatically schedule and launch it as soon as resources become available. No need for manual retries: just submit your job and let the system handle the rest.   </p>"},{"location":"job_submission/k8s-jobs/#42-submit-your-first-kubernetes-job","title":"4.2) Submit your first Kubernetes Job","text":"<p>Submitting a job is simple and flexible, but assumes your application is packaged as a Docker container.  You can use third-party public images (e.g. from DockerHub) or build and push your own image to our private container image registry, as explained in this guide.  </p> <p>Please note that, once the job is running, you\u2019ll be able to open a terminal inside the container, just like SSH access, allowing for full interaction with your running workloads.</p>"},{"location":"job_submission/k8s-jobs/#enough-chit-chat-lets-submit-our-jobs","title":"Enough chit-chat, let's submit our Jobs!","text":"<p>Before submitting any job, ensure that you have configured kubectl and your access token is valid. See Section 3: Configuring Access for details.</p> <p>Create a text file (e.g., my-job.yaml) with the following content:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: &lt;job-name&gt;\nspec:\n  ttlSecondsAfterFinished: 604800\n  backoffLimit: 0\n  template:\n    spec:\n#      runtimeClassName: nvidia\n      containers:\n      - name: &lt;container-name&gt;\n        image: &lt;container-image&gt;\n        command: [\"sh\", \"-c\", \"&lt;command_1&gt;; &lt;command_2&gt;; &lt;command_3&gt;; ...\"]\n        resources:\n          requests:\n            cpu: \"0.2\"\n            memory: \"100Mi\"\n          limits:\n            cpu: \"&lt;integer&gt;\"\n            memory: \"&lt;integer&gt;Gi\"\n            nvidia.com/gpu: 0\n        volumeMounts:\n        - name: lustre\n          mountPath: /lustre\n        - name: lustrehome\n          mountPath: /lustrehome\n      restartPolicy: Never\n      volumes:\n      - name: lustre\n        hostPath:\n          path: /lustre\n          type: Directory\n      - name: lustrehome\n        hostPath:\n          path: /lustrehome\n          type: Directory\n#      nodeSelector:\n#        nvidia.com/gpu.product: Tesla-V100-PCIE-32GB\n#        nvidia.com/gpu.product: NVIDIA-A100-PCIE-40GB\n#        nvidia.com/gpu.product: NVIDIA-H100-80GB-HBM3\n</code></pre> <p>You\u2019ll need to replace these placeholder values: </p> <p>metadata.name: A unique name for your Job;</p> <p></p> <p>spec.backoffLimit:  Specifies the number of retry attempts Kubernetes will make if the Job fails before giving up. Once the number of failures reaches this limit, the Job is marked as failed and will not be retried further. For most jobs, it makes sense to leave <code>backoffLimit: 0</code> as is \u2014 no need to restart the job upon failure. If needed, you can increase this value up to a maximum of 6 retry attempts. If the field is omitted, it will default to 6.</p> <p></p> <p>spec.ttlSecondsAfterFinished: Duration in seconds to retain the Job object in the cluster after it completes execution, either successfully or with an error. After this period of time, the Job is automatically deleted by the TTL controller. Put simply, this setting controls how long the system keeps logs and related job information after the job has finished or failed. In this manifest, the value is set to one week (604800 seconds). This setting only affects the Job controller object, not the container lifecycle. The Pods created by the Job will still terminate as usual upon completion, the containers will not continue running during this TTL period. However, logs and information regarding the terminated Pods and Jobs remain accessible for as long as specified in this field. Feel free to adjust this parameter as preferred, up to a maximum of 3 months (67737600 seconds).  </p> <p></p> <p>spec.template.spec.containers.name: Arbitrary name for your container. In Kubernetes, a Job is an object that wraps one or more containers: that's why you need to specify both a name for the Job object and one for the container(s) the Job launches;</p> <p></p> <p>spec.template.spec.containers.image: Container image. In case it is a public one directly  from DockerHub you only need to specify the image name or link, so for example  </p> <p><code>image: ubuntu</code> </p> <p>or</p> <p><code>image: gcr.io/google-containers/pause:3.9</code> </p> <p>Otherwise, if you are launching a container with a custom image pushed in our private image registry, you need to specify it like  </p> <p><code>image: registry-clustergpu.recas.ba.infn.it/{yourUsername}/{yourImage}</code> </p> <p>So, for example  </p> <p><code>image: registry-clustergpu.recas.ba.infn.it/gvino/cuda11.5.0-base-ubuntu20.04:0.1</code></p> <p></p> <p>spec.template.spec.containers.resources.limits.cpu, spec.template.spec.containers.resources.limits.memory and spec.template.spec.containers.resources.limits.nvidia.com/gpu: limits to the number of CPU cores, RAM in terms of Gibibytes (\u2248 Gigabytes) and number of GPU(s). The parameters under resources.requests, instead, define the minimum guaranteed amount of resources that the container will receive. For most use cases, the default values we have provided should be sufficient and typically don\u2019t require adjustment.</p> <p></p> <p>spec.template.spec.containers.command: overrides the image 'ENTRYPOINT' field; This is typically where you want to put the command(s) to start the container with. For example, to execute <code>echo Starting job...</code> followed by <code>sleep 3600</code>:</p> <p><code>command: [\"sh\", \"-c\", \"echo Starting job...; sleep 3600\"]</code> </p> <p></p> <p>spec.template.spec.containers.args: overrides the image 'CMD' field.</p> <p>Note For a quick overview on the 'CMD' and 'ENTRYPOINT' fields of a container image, please check this link.</p> <p></p> <p>If your workload does not require a GPU, please leave <code>spec.runtimeClassName</code> and the entire <code>spec.nodeSelector</code> section commented out. If, instead, your workload does require the use of GPUs, change the <code>nvidia.com/gpu</code> limit and uncomment (just remove the '#' from the manifest and leave indentation as is) the <code>spec.runtimeClassName</code> field, the <code>nodeSelector:</code> line and JUST its subfield regarding the kind of GPU you are interested in using (whether a NVIDIA V100/A100/H100). Please note that your request can end up in error if you:  </p> <ul> <li>are not allowed to use that given kind of GPU;  </li> <li>are requesting GPUs and leaving the <code>runtimeClassName</code> and <code>nodeSelector</code> fields commented out.</li> </ul> <p>For reference, here are two fully avvalorated examples for a Job not requesting a GPU (first manifest) and a Job requesting 2 NVIDIA A100 GPUs:</p> <p></p>"},{"location":"job_submission/k8s-jobs/#example-job-not-requesting-a-gpu","title":"Example Job NOT requesting a GPU","text":"<p><pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: documentazione-job\nspec:\n  ttlSecondsAfterFinished: 604800\n  backoffLimit: 0\n  template:\n    spec:\n#      runtimeClassName: nvidia       # &lt;&lt;&lt; \ud83d\udd34 Commented out\n      containers:\n      - name: documentazione-job\n        image: busybox\n        command: [\"sh\", \"-c\", \"echo Starting job...; sleep 3600\"]\n        resources:\n          requests:\n            cpu: \"0.2\"\n            memory: \"100Mi\"        \n          limits:\n            cpu: \"2\"\n            memory: \"4Gi\"\n            nvidia.com/gpu: 0         # &lt;&lt;&lt; \ud83d\udd34 Set to 0\n        volumeMounts:\n        - name: lustre\n          mountPath: /lustre\n        - name: lustrehome\n          mountPath: /lustrehome\n      restartPolicy: Never\n      volumes:\n      - name: lustre\n        hostPath:\n          path: /lustre\n          type: Directory\n      - name: lustrehome\n        hostPath:\n          path: /lustrehome\n          type: Directory\n#      nodeSelector:                                          # &lt;&lt;&lt; \ud83d\udd34 Commented out\n#        nvidia.com/gpu.product: Tesla-V100-PCIE-32GB         # &lt;&lt;&lt; \ud83d\udd34 Commented out\n#        nvidia.com/gpu.product: NVIDIA-H100-80GB-HBM3        # &lt;&lt;&lt; \ud83d\udd34 Commented out\n#        nvidia.com/gpu.product: NVIDIA-A100-PCIE-40GB        # &lt;&lt;&lt; \ud83d\udd34 Commented out\n</code></pre> </p>"},{"location":"job_submission/k8s-jobs/#example-job-requesting-2-nvidia-a100-gpus","title":"Example Job requesting 2 NVIDIA A100 GPUs","text":"<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: documentazione-job\nspec:\n  ttlSecondsAfterFinished: 604800\n  backoffLimit: 0                     \n  template:\n    spec:\n      runtimeClassName: nvidia        # &lt;&lt;&lt; \ud83d\udfe2 Uncommented\n      containers:\n      - name: documentazione-job\n        image: busybox\n        command: [\"sh\", \"-c\", \"echo Starting job...; sleep 60\"]\n        resources:\n          requests:\n            cpu: \"0.2\"\n            memory: \"100Mi\"        \n          limits:\n            cpu: \"2\"\n            memory: \"4Gi\"\n            nvidia.com/gpu: 2         # &lt;&lt;&lt; \ud83d\udfe2 Set to integer\n        volumeMounts:\n        - name: lustre\n          mountPath: /lustre\n        - name: lustrehome\n          mountPath: /lustrehome\n      restartPolicy: Never\n      volumes:\n      - name: lustre\n        hostPath:\n          path: /lustre\n          type: Directory\n      - name: lustrehome\n        hostPath:\n          path: /lustrehome\n          type: Directory\n      nodeSelector:                                           # &lt;&lt;&lt; \ud83d\udfe2 Uncommented    \n#        nvidia.com/gpu.product: Tesla-V100-PCIE-32GB         # &lt;&lt;&lt; \ud83d\udd34 Commented out (I want 2 A100)\n#        nvidia.com/gpu.product: NVIDIA-H100-80GB-HBM3        # &lt;&lt;&lt; \ud83d\udd34 Commented out (I want 2 A100)\n        nvidia.com/gpu.product: NVIDIA-A100-PCIE-40GB         # &lt;&lt;&lt; \ud83d\udfe2 Uncommented\n</code></pre> <p>Once your manifest is ready and saved (e.g. as my-job.yaml), submit it to the Kubernetes cluster with:  </p> <p><code>kubectl create -f /path/to/file</code> </p> <p>so, for example  </p> <p><code>kubectl create -f my-job.yaml</code></p> <p>Kubernetes will validate the manifest and either start the job immediately (if resources are available), queue it for execution when the required resources become free or return an error if the file is incorrectly written.</p> <p>Note So far, we've simplified things by saying that a Kubernetes Job is a wrapper around a container. That\u2019s not entirely accurate. In Kubernetes, the smallest deployable unit is the Pod\u2014not the container itself. A Pod typically wraps one container (or sometimes more) and provides the execution environment for it. A Job, in turn, is a controller that manages the lifecycle of one or more Pods and ensures that a specified task completes successfully. For example, it can enforce a maximum runtime for the underlying Pod. In practical terms: when you submit a Job to the cluster, Kubernetes creates a Pod, and that Pod runs the container you defined for your workload. We won\u2019t dive deeper into the details of Pods in this guide, as they\u2019re not essential for day-to-day usage. However, if you're curious, you can read more in the official Kubernetes documentation. </p>"},{"location":"job_submission/k8s-jobs/#43-submitting-more-than-a-job-resubmitting-the-same-job","title":"4.3) Submitting more than a Job / Resubmitting the same job","text":"<p>In Kubernetes, the unique identifier for an object is its name, combined with the namespace it resides in. Since each user operates within their own dedicated namespace, only one object of a given type (e.g., a Job) with a specific name can exist at a time within that namespace.</p> <p>This means that in order to submit multiple Jobs starting from the same manifest file, you must either change the metadata.name field of the Job in the YAML definition for each submission, ensuring uniqueness, or delete the previous Job before re-submitting a new one with the same name:</p> <pre><code>kubectl delete -f /path/to/job/definition.yaml\nkubectl create -f /path/to/job/definition.yaml\n</code></pre>"},{"location":"job_submission/k8s-jobs/#44-shared-storage","title":"4.4) Shared storage","text":"<p>In the Job manifest file, you might have noticed the following section:</p> <pre><code>        volumeMounts:\n        - name: lustre\n          mountPath: /lustre\n        - name: lustrehome\n          mountPath: /lustrehome\n      restartPolicy: Never\n      volumes:\n      - name: lustre\n        hostPath:\n          path: /lustre\n          type: Directory\n      - name: lustrehome\n        hostPath:\n          path: /lustrehome\n          type: Directory\n</code></pre> <p>This configuration ensures that the shared storage available on the ReCaS frontend (frontend.recas.ba.infn.it) is also mounted inside your container. As a result, all the files you see on the frontend (under /lustre and /lustrehome) will also be accessible from within your running container\u2014automatically. Any changes you make to files within these mounted paths from inside the container \u2014 such as creating, editing, or deleting files \u2014 will be immediately reflected on the actual storage on the frontend. In practice, it\u2019s as if you were working directly on the frontend itself, but within an isolated container environment.</p> <p>Note Proper file permissions and access control are enforced. So don\u2019t try anything sneaky \u2014 you will only be able to access files and directories you are authorized to.</p>"},{"location":"job_submission/k8s-jobs/#45-monitoring-and-debugging-submitted-jobs","title":"4.5) Monitoring and Debugging Submitted Jobs","text":"<p>Just because your kubectl create command doesn't return an error doesn't mean the Job actually ran successfully.  To monitor the state of your Job and the underlying Pod, use the following commands: </p> <ul> <li>List your pods:   </li> </ul> <p> <code>kubectl get pod</code></p> <ul> <li>Describe a specific pod (helpful to debug scheduling issues or container errors): </li> </ul> <p> <code>kubectl describe pod &lt;podName&gt;</code></p> <ul> <li>Check logs from the pod (only available after the container has started running; shows the standard output and error streams of the software running inside the container, useful for debugging and viewing runtime logs): </li> </ul> <p> <code>kubectl logs &lt;podName&gt;</code></p> <ul> <li>List your jobs:  </li> </ul> <p> <code>kubectl get job</code> </p> <ul> <li>Inspect job details:  </li> </ul> <p> <code>kubectl describe job &lt;jobName&gt;</code></p> <p>These commands will help you identify what went wrong, whether the container failed to start, exited early or encountered runtime errors.</p> <p>Note Logs and information about completed (or failed) Jobs and their Pods are only available for as long as specified by the ttlSecondsAfterFinished setting in the Job manifest. After this time, the Job is automatically deleted and its related data is no longer accessible. If you want to keep logs and other information permanently, you have two options. One is to remove the ttlSecondsAfterFinished field from the Job manifest, which would prevent the Job from being automatically deleted. However, this is not recommended, as over time your namespace would fill up with old completed Jobs, making it harder to manage and requiring manual cleanup.  The recommended approach is instead to save the information you need\u2014such as pod descriptions or logs\u2014to a file using commands like <code>kubectl describe pod &lt;podName&gt; &gt; /path/where/to/store/the/log/file</code> and, similarly, for all the other commands listed above. This way, you can retain any important details without cluttering the cluster.  </p>"},{"location":"job_submission/k8s-jobs/#46-opening-a-terminal-into-a-container","title":"4.6) Opening a terminal into a container","text":"<p>If your job is running and you want to interact directly with the container \u2014 for example to run commands, inspect files or debug issues \u2014 you can run a command inside it using:  </p> <p><code>kubectl exec -it &lt;podName&gt; -- &lt;command&gt;</code> </p> <p>Note If your pod has more than one container, you will also need to specify which container you want to access: <code>kubectl exec -it &lt;podName&gt; -c &lt;containerName&gt; -- &lt;command&gt;</code></p> <p>A common use case may be running a terminal inside your container. If the bash shell is installed inside the container:  </p> <p><code>kubectl exec -it &lt;podName&gt; -- bash</code> </p> <p>if instead bash is not present in the image you can try with  </p> <p><code>kubectl exec -it &lt;podName&gt; -- sh</code> </p>"},{"location":"job_submission/k8s-jobs/#47-deleting-a-job","title":"4.7) Deleting a job","text":"<p>If you submitted a job by mistake, or if you noticed there is something wrong with your workload, you can delete a Job manually to free up cluster resources and keep your environment clean.  </p> <p>There are two main ways to delete a Job:  </p> <p><code>kubectl delete job &lt;job-name&gt;</code> </p> <p>or  </p> <p><code>kubectl delete -f /path/to/Job/definitionFile</code> </p> <p>e.g.  </p> <p><code>kubectl delete -f my-job.yaml</code></p> <p>Both of these approaches will also delete the associated Pod(s) created by that Job.  </p> <p>Note Of course, deleting a Job does not delete any files that may have been written to the shared volumes like /lustre or /lustrehome. Any data you write to these directories from within the container is persisted \u2014 just as if you were working directly on the frontend.</p>"}]}